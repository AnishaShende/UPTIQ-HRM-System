{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a480f6f3",
   "metadata": {},
   "source": [
    "# RAG Ecosystem\n",
    "\n",
    "[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/) [![LangChain](https://img.shields.io/badge/LangChain-%23007ACC.svg?logo=LangChain)](https://www.langchain.com/) [![DeepEval](https://img.shields.io/badge/DeepEval-Evaluation-orange)](https://github.com/confident-ai/deepeval) [![RAGAS](https://img.shields.io/badge/RAGAS-Evaluation-blueviolet)](https://github.com/explodinggradients/ragas) [![OpenAI](https://img.shields.io/badge/OpenAI-API-lightgrey)](https://openai.com/) [![Cohere](https://img.shields.io/badge/Cohere-API-yellowgreen)](https://cohere.com/) [![Medium](https://img.shields.io/badge/Medium-Blog-black?logo=medium)](https://medium.com/@fareedkhandev/8f23349b96a4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2a958b",
   "metadata": {},
   "source": [
    "Creating an entire RAG based AI system depends on many different components with each requires it’s own optimization and careful implementation. These components includes:\n",
    "\n",
    "![Production Ready RAG System (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:2400/1*ZjozYulECfqrzgMaTEZ-Rg.png)\n",
    "\n",
    "- **Query Transformations:** Rewriting user questions to be more effective for retrieval.\n",
    "- **Intelligent Routing:** Directing a query to the correct data source or a specialized tool.\n",
    "- **Indexing:** Creating a multi-layered knowledge base.\n",
    "- **Retrieval and Re-ranking:** Filtering noise and prioritizing the most relevant context.\n",
    "- **Self-Correcting Agentic Flows:** Building systems that can grade and improve their own work.\n",
    "- **End-to-End Evaluation:** Objectively measuring the performance of the entire pipeline.\n",
    "\n",
    "and much more …\n",
    "\n",
    "> We will learn and code each part of the RAG ecosystem along with visuals for easier understanding, starting from the basics to advanced techniques.\n",
    "\n",
    "All the code (Theory + Notebook) is available in my GitHub Repo:\n",
    "\n",
    "[[[[[[[[[[[[[[ LINK ]]]]]]]]]]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893b98a0",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Understanding Basic RAG System](#part1)\n",
    "  - [Indexing Phase](#part1-1)\n",
    "  - [Retrieval](#part1-2)\n",
    "  - [Generation](#part1-3)\n",
    "- [Advanced Query Transformations](#part2)\n",
    "  - [Multi-Query Generation](#part2-1)\n",
    "  - [RAG-Fusion](#part2-2)\n",
    "  - [Decomposition](#part2-3)\n",
    "  - [Step-Back Prompting](#part2-4)\n",
    "  - [HyDE](#part2-5)\n",
    "- [Routing & Query Construction](#part3)\n",
    "  - [Logical Routing](#part3-1)\n",
    "  - [Semantic Routing](#part3-2)\n",
    "  - [Query Structuring](#part3-3)\n",
    "- [Advanced Indexing Strategies](#part4)\n",
    "  - [Multi-Representation Indexing](#part4-1)\n",
    "  - [Hierarchical Indexing (RAPTOR) Knowledge Tree](#part4-2)\n",
    "  - [Token-Level Precision (ColBERT)](#part4-3)\n",
    "- [Advanced Retrieval & Generation](#part5)\n",
    "  - [Dedicated Re-ranking](#part5-1)\n",
    "  - [Self-Correction using AI Agents](#part5-2)\n",
    "  - [Impact of Long Context](#part5-3)\n",
    "- [Manual RAG Evaluation](#part6)\n",
    "  - [The Core Metrics: What Should We Measure?](#part6-1)\n",
    "  - [Building Evaluators from Scratch with LangChain](#part6-2)\n",
    "- [Evaluation with Frameworks](#part7)\n",
    "  - [Rapid Evaluation with deepeval](#part7-1)\n",
    "  - [Another Powerful Alternative with grouse](#part7-2)\n",
    "  - [Evaluation with RAGAS](#part7-3)\n",
    "- [Summarizing Everything](#part8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5610337",
   "metadata": {},
   "source": [
    "<a id='part1'></a>\n",
    "# Understanding Basic RAG System\n",
    "\n",
    "Before we look into the basics of RAG, let’s install core Python libraries commonly used for AI products, such as LangChain and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83844704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./myenv/lib/python3.12/site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain_community in ./myenv/lib/python3.12/site-packages (0.3.29)\n",
      "Requirement already satisfied: langchain-openai in ./myenv/lib/python3.12/site-packages (0.3.33)\n",
      "Requirement already satisfied: langchainhub in ./myenv/lib/python3.12/site-packages (0.1.21)\n",
      "Requirement already satisfied: chromadb in ./myenv/lib/python3.12/site-packages (1.0.21)\n",
      "Requirement already satisfied: tiktoken in ./myenv/lib/python3.12/site-packages (0.11.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in ./myenv/lib/python3.12/site-packages (from langchain) (0.3.76)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in ./myenv/lib/python3.12/site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in ./myenv/lib/python3.12/site-packages (from langchain) (0.4.27)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./myenv/lib/python3.12/site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./myenv/lib/python3.12/site-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in ./myenv/lib/python3.12/site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./myenv/lib/python3.12/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./myenv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./myenv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in ./myenv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./myenv/lib/python3.12/site-packages (from langchain_community) (3.12.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in ./myenv/lib/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in ./myenv/lib/python3.12/site-packages (from langchain_community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./myenv/lib/python3.12/site-packages (from langchain_community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in ./myenv/lib/python3.12/site-packages (from langchain_community) (2.3.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./myenv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./myenv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.6.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in ./myenv/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./myenv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.104.2 in ./myenv/lib/python3.12/site-packages (from langchain-openai) (1.107.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./myenv/lib/python3.12/site-packages (from tiktoken) (2025.9.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (0.10.0)\n",
      "Requirement already satisfied: sniffio in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./myenv/lib/python3.12/site-packages (from openai<2.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: httpcore==1.* in ./myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.104.2->langchain-openai) (0.16.0)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in ./myenv/lib/python3.12/site-packages (from langchainhub) (2.32.4.20250809)\n",
      "Requirement already satisfied: build>=1.0.3 in ./myenv/lib/python3.12/site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in ./myenv/lib/python3.12/site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./myenv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in ./myenv/lib/python3.12/site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./myenv/lib/python3.12/site-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./myenv/lib/python3.12/site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./myenv/lib/python3.12/site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./myenv/lib/python3.12/site-packages (from chromadb) (1.37.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./myenv/lib/python3.12/site-packages (from chromadb) (0.22.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./myenv/lib/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./myenv/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./myenv/lib/python3.12/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./myenv/lib/python3.12/site-packages (from chromadb) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./myenv/lib/python3.12/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./myenv/lib/python3.12/site-packages (from chromadb) (0.17.4)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./myenv/lib/python3.12/site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./myenv/lib/python3.12/site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./myenv/lib/python3.12/site-packages (from chromadb) (3.11.3)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./myenv/lib/python3.12/site-packages (from chromadb) (14.1.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in ./myenv/lib/python3.12/site-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: six>=1.5 in ./myenv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in ./myenv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./myenv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: pyproject_hooks in ./myenv/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./myenv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./myenv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./myenv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in ./myenv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./myenv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./myenv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in ./myenv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in ./myenv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./myenv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./myenv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./myenv/lib/python3.12/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./myenv/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
      "Requirement already satisfied: coloredlogs in ./myenv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./myenv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in ./myenv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (6.32.1)\n",
      "Requirement already satisfied: sympy in ./myenv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in ./myenv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./myenv/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in ./myenv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in ./myenv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.37.0 in ./myenv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.37.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in ./myenv/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.58b0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./myenv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./myenv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./myenv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./myenv/lib/python3.12/site-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.9)\n",
      "Requirement already satisfied: click>=8.0.0 in ./myenv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./myenv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./myenv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in ./myenv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./myenv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./myenv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./myenv/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing Required Modules\n",
    "!pip install langchain langchain_community langchain-openai langchainhub chromadb tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0517d6a4",
   "metadata": {},
   "source": [
    "We can now simply set the environment variables for tracing and other tasks, such as the LLMs API provider we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b98d5f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set LangChain API endpoint and API key for tracing with LangSmith\n",
    "# os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "# os.environ['LANGCHAIN_API_KEY'] = '<your-api-key>'  # Replace with your LangChain API key\n",
    "\n",
    "# Set OpenAI API key for using OpenAI models\n",
    "# os.environ['OPENAI_API_KEY'] = '<your-api-key>'  # Replace with your OpenAI API key\n",
    "os.environ['GEMINI_API_KEY'] = 'AIzaSyANRVAIhB209L-LUL0pAVulj-BhI7roHBw'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091c051a",
   "metadata": {},
   "source": [
    "You can obtain your `LangSmith` API key from [their official documentation](https://www.langchain.com/langsmith) to trace our RAG product throughout this blog. For the LLM, we will be using the `OpenAI` API but as you may already know, `LangChain` supports a variety of LLM providers as well.\n",
    "\n",
    "The core RAG pipeline is the foundation of any advanced system, and understanding its components is important. Therefore, before going into the details of advanced components, we first need to understand the core logic of how a RAG system works, **but you can skip this section if you are already aware of how RAG system works.**\n",
    "\n",
    "![Basic RAG system (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*c_yxo0cUH8u7o5an-Tzi0g.png)\n",
    "\n",
    "This simplest RAG can be break into three components:\n",
    "\n",
    "- **Indexing**: Organize and store data in a structured format to enable efficient searching.\n",
    "- **Retrieval**: Search and fetch relevant data based on a query or input.\n",
    "- **Generation**: Create a final response or output using the retrieved data.\n",
    "\n",
    "Let’s build this simple pipeline from the ground up to see how each piece works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100be33",
   "metadata": {},
   "source": [
    "<a id='part1-1'></a>\n",
    "## Indexing Phase\n",
    "\n",
    "Before our RAG system can answer any questions, it needs knowledge to draw from. For this, we’ll use a `WebBaseLoader` to pull content directly from [Lilian Weng's excellent blog post](https://lilianweng.github.io/posts/2023-06-23-agent/) on LLM-powered agents.\n",
    "\n",
    "![Indexing phase (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:875/1*dnSg_QmGd4J030_bznvUPw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fbbf0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 HR policy documents\n",
      "1. employee_code_of_conduct.txt: 2595 characters\n",
      "2. leave_policy.txt: 2044 characters\n",
      "3. it_and_security_policy.txt: 1600 characters\n",
      "4. payroll_and_compensation_policy.txt: 1610 characters\n",
      "5. work_from_home_policy.txt: 1616 characters\n",
      "6. performance_review_policy.txt: 1453 characters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path to your HR policies folder\n",
    "hr_policies_path = \"/home/anisha/Desktop/Projects/hrms_fullstack/uptiqai-hrm-system/Human-Resource-Management-System/rag/uptiq_hr_policies\"\n",
    "\n",
    "# Load all text files from the HR policies directory\n",
    "loader = DirectoryLoader(\n",
    "    hr_policies_path,\n",
    "    glob=\"*.txt\",  # Load all .txt files\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "\n",
    "# Load the documents\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} HR policy documents\")\n",
    "for i, doc in enumerate(docs):\n",
    "    filename = os.path.basename(doc.metadata['source'])\n",
    "    print(f\"{i+1}. {filename}: {len(doc.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a641e",
   "metadata": {},
   "source": [
    "The `bs_kwargs` argument helps us target only the relevant HTML tags (`post-content`, `post-title`, etc.), cleaning up our data from the start.\n",
    "\n",
    "Now that we have the document, we face our first challenge. Feeding a massive document directly into an LLM is inefficient and often impossible due to context window limits.\n",
    "\n",
    "> This is why **chunking** is a critical step. We need to break the document into smaller, semantically meaningful pieces.\n",
    "\n",
    "The `RecursiveCharacterTextSplitter` is the recommended tool for this job because it intelligently tries to keep paragraphs and sentences intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac6bab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a text splitter to divide text into chunks of 1000 (200) characters with 200-character (20) overlap\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "\n",
    "# Split the loaded documents into smaller chunks\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a32468",
   "metadata": {},
   "source": [
    "With `chunk_size=1000`, we are creating chunks of 1000 characters, and `chunk_overlap=200` ensures there is some continuity between them, which helps preserve context.\n",
    "\n",
    "Our text is now split, but it’s still just text. To perform similarity searches, we need to convert these chunks into numerical representations called **embeddings**. We will then store these embeddings in a **vector store**, which is a specialized database designed for efficient searching of vectors.\n",
    "\n",
    "The `Chroma` vector store and `OpenAIEmbeddings` make this incredibly simple. The following line handles both embedding and indexing in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e9bf704-08c7-4c01-a92f-6c274460caa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_google_genai\n",
      "  Downloading langchain_google_genai-2.1.10-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain_google_genai)\n",
      "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.75 in ./myenv/lib/python3.12/site-packages (from langchain_google_genai) (0.3.76)\n",
      "Requirement already satisfied: pydantic<3,>=2 in ./myenv/lib/python3.12/site-packages (from langchain_google_genai) (2.11.7)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai)\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in ./myenv/lib/python3.12/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.40.3)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in ./myenv/lib/python3.12/site-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (6.32.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in ./myenv/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in ./myenv/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in ./myenv/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (1.74.0)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai)\n",
      "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./myenv/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./myenv/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./myenv/lib/python3.12/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (4.9.1)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in ./myenv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (0.4.27)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./myenv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./myenv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./myenv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./myenv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in ./myenv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (24.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./myenv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./myenv/lib/python3.12/site-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./myenv/lib/python3.12/site-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./myenv/lib/python3.12/site-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (2025.8.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./myenv/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain_google_genai) (0.6.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (0.24.0)\n",
      "Requirement already satisfied: anyio in ./myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./myenv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.75->langchain_google_genai) (1.3.1)\n",
      "Downloading langchain_google_genai-2.1.10-py3-none-any.whl (49 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m353.4 kB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m8.8 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Downloading grpcio_status-1.74.0-py3-none-any.whl (14 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Installing collected packages: filetype, proto-plus, grpcio-status, google-api-core, google-ai-generativelanguage, langchain_google_genai\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [langchain_google_genai]\u001b[32m4/6\u001b[0m [google-ai-generativelanguage]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 google-api-core-2.25.1 grpcio-status-1.74.0 langchain_google_genai-2.1.10 proto-plus-1.26.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e824e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Embed the text chunks and store them in a Chroma vector store for similarity search\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,  \n",
    "    embedding=OpenAIEmbeddings()  # Use OpenAI's embedding model to convert text into vectors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81e7aa58-b0c4-4456-9d1f-1c49ff8980d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_huggingface\n",
      "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in ./myenv/lib/python3.12/site-packages (from langchain_huggingface) (0.3.76)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in ./myenv/lib/python3.12/site-packages (from langchain_huggingface) (0.22.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.4 in ./myenv/lib/python3.12/site-packages (from langchain_huggingface) (0.34.4)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (0.4.27)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (4.15.0)\n",
      "Requirement already satisfied: packaging>=23.2 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (24.2)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in ./myenv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./myenv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./myenv/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (2025.9.0)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./myenv/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./myenv/lib/python3.12/site-packages (from huggingface-hub>=0.33.4->langchain_huggingface) (1.1.9)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./myenv/lib/python3.12/site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (0.24.0)\n",
      "Requirement already satisfied: anyio in ./myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (4.10.0)\n",
      "Requirement already satisfied: certifi in ./myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (1.0.9)\n",
      "Requirement already satisfied: idna in ./myenv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./myenv/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./myenv/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./myenv/lib/python3.12/site-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.33.4->langchain_huggingface) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests->huggingface-hub>=0.33.4->langchain_huggingface) (2.5.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./myenv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain_huggingface) (1.3.1)\n",
      "Downloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: langchain_huggingface\n",
      "Successfully installed langchain_huggingface-0.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce23883a-39b6-4354-bb90-416c8495c088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Requirement already satisfied: tqdm in ./myenv/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./myenv/lib/python3.12/site-packages (from sentence-transformers) (0.34.4)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Downloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in ./myenv/lib/python3.12/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./myenv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./myenv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./myenv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./myenv/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.0)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./myenv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./myenv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.9)\n",
      "Requirement already satisfied: setuptools in ./myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m  \u001b[33m0:02:55\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m  \u001b[33m0:01:26\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:04\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m  \u001b[33m0:00:33\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m  \u001b[33m0:02:24\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m  \u001b[33m0:00:28\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m  \u001b[33m0:00:34\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m  \u001b[33m0:00:36\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m  \u001b[33m0:01:35\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m  \u001b[33m0:01:30\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:05\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━\u001b[0m \u001b[32m280.8/322.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:10\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Connection timed out while downloading.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Attempting to resume incomplete download (280.8 MB/322.4 MB, attempt 1)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1f5a7fb0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d43f830>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d3c5b20>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d3c6db0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d44dbe0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Attempting to resume incomplete download (280.8 MB/322.4 MB, attempt 2)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d44f650>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d44fcb0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d44efc0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d44ec30>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d44ea50>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Attempting to resume incomplete download (280.8 MB/322.4 MB, attempt 3)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d44fda0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d44e330>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d3c5190>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d3c5b20>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d43f560>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0mResuming download nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (280.8 MB/322.4 MB)\n",
      "\u001b[2K   \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━\u001b[0m \u001b[32m282.1/322.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:13\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Connection timed out while downloading.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Attempting to resume incomplete download (282.1 MB/322.4 MB, attempt 4)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d43f9b0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d44db20>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d43e1b0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x741d1d43e510>: Failed to establish a new connection: [Errno -2] Name or service not known')': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
      "\u001b[0mResuming download nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (282.1 MB/322.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m  \u001b[33m0:00:24\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m3.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading scipy-1.16.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, triton, threadpoolctl, scipy, safetensors, Pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, joblib, scikit-learn, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, torch, sentence-transformers\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25/25\u001b[0m [sentence-transformers]25\u001b[0m [sentence-transformers]-cu12]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed Pillow-11.3.0 joblib-1.5.2 networkx-3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 sentence-transformers-5.1.0 threadpoolctl-3.6.0 torch-2.8.0 transformers-4.56.1 triton-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62bbdf63-ca20-4c80-9c5f-7e7a051761a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Using sentence-transformers model (runs locally, completely free)\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=HuggingFaceEmbeddings(\n",
    "        model_name=\"all-MiniLM-L6-v2\"  # Fast and efficient model\n",
    "        # model_name=\"all-mpnet-base-v2\"  # Better quality but slower\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2192e6",
   "metadata": {},
   "source": [
    "With our knowledge indexed, we are now ready to start asking questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc733ac",
   "metadata": {},
   "source": [
    "<a id='part1-2'></a>\n",
    "## Retrieval\n",
    "\n",
    "The vector store is our library, and the **retriever** is our smart librarian. It takes a user’s query, embeds it, and then fetches the most semantically similar chunks from the vector store.\n",
    "\n",
    "![Retrieval Phase (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*jtf1FoBGfpnDPTTu9N94Wg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4b59828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3627ad",
   "metadata": {},
   "source": [
    "Let’s test it. We’ll ask a question and see what our retriever finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9fb8243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Salary Structure\n",
      "   - Basic Salary\n",
      "   - House Rent Allowance (HRA)\n",
      "   - Special Allowances\n",
      "   - Performance Bonus\n",
      "   - Provident Fund & Gratuity (as per statutory regulations)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28205/449673751.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(\"What is Salary Structure?\")\n"
     ]
    }
   ],
   "source": [
    "# Retrieve relevant documents for a query\n",
    "docs = retriever.get_relevant_documents(\"What is Salary Structure?\")\n",
    "\n",
    "# Print the content of the first retrieved document\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3272bc",
   "metadata": {},
   "source": [
    "As you can see, the retriever successfully pulled the most relevant chunk from the blog post that directly discusses “Task decomposition.” This piece of context is exactly what the LLM needs to form an accurate answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42d28c",
   "metadata": {},
   "source": [
    "<a id='part1-3'></a>\n",
    "## Generation\n",
    "\n",
    "We have our context, but we need an LLM to read it and formulate a human-friendly answer. This is the **“Generation”** step in RAG.\n",
    "\n",
    "![Generation Step (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*0K6ognTAEOJQmb6KDL9wBw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edadc16b",
   "metadata": {},
   "source": [
    "First, we need a good prompt template. This instructs the LLM on how to behave. Instead of writing our own, we can pull a pre-optimized one from LangChain Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5f72b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "# Pull a pre-made RAG prompt from LangChain Hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# printing the prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dbe226",
   "metadata": {},
   "source": [
    "Next, we initialize our LLM. We’ll use `gpt-3.5-turbo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# # Initialize the LLM\n",
    "# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45ec9754-7f12-490f-a4b1-697b467ef6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e584760-e2c8-4823-b9f4-c9232ea76407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79956469-582f-40a0-b2a5-d98572a61f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded9935",
   "metadata": {},
   "source": [
    "Now for the final step: chaining everything together. Using the LangChain Expression Language (LCEL), we can pipe the output of one component into the input of the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbeb053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Helper function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Define the full RAG chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d9050",
   "metadata": {},
   "source": [
    "Let’s break down this chain:\n",
    "\n",
    "1. `{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}`: This part runs in parallel. It sends the user's question to the `retriever` to get documents, which are then formatted into a single string by `format_docs`. Simultaneously, `RunnablePassthrough` passes the original question through unchanged.\n",
    "2. `| prompt`: The context and question are fed into our prompt template.\n",
    "3. `| llm`: The formatted prompt is sent to the LLM.\n",
    "4. `| StrOutputParser()`: This cleans up the LLM's output into a simple string.\n",
    "\n",
    "Now, let’s invoke the entire chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "511eb59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A salary structure is a breakdown of an employee's total compensation, divided into components such as Basic Salary, House Rent Allowance (HRA), Special Allowances, Performance Bonus, and Provident Fund & Gratuity. This structure ensures fair and transparent pay practices, aligned with legal and market standards. It helps employees understand their compensation composition.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question using the RAG chain\n",
    "response = rag_chain.invoke(\"What is Salary Structure?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71edc1",
   "metadata": {},
   "source": [
    "And there we have it, our RAG pipeline successfully retrieved relevant information about **“Task Decomposition”** and used it to generate a concise, accurate answer. This simple chain forms the foundation upon which we will build more advanced and powerful capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1833de",
   "metadata": {},
   "source": [
    "<a id='part2'></a>\n",
    "# Advanced Query Transformations\n",
    "\n",
    "So, now that we understand the fundamentals of RAG pipeline. But production systems often reveal the limitations of this basic approach. One of the most common failure points is the user’s query itself.\n",
    "\n",
    "![Query Transformation (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*FO2U9QA49kjn6OaBGZuq8A.png)\n",
    "\n",
    "> A query might be too specific, too broad, or use different vocabulary than our source documents, leading to poor retrieval results.\n",
    "\n",
    "The solution isn’t to blame the user, it’s to make our system smarter. **Query Transformation** is a set of powerful techniques designed to re-write, expand, or break down the original question to significantly improve retrieval accuracy.\n",
    "\n",
    "Instead of relying on a single query, we’ll engineer multiple, better-informed queries to cast a wider and more accurate net.\n",
    "\n",
    "To test these new techniques, we will use the same indexed knowledge base from Basic RAG pipeline section that we have just gone through previously. This ensures we can directly compare the results and see the improvements.\n",
    "\n",
    "As a quick refresher, here’s how we set up our retriever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c2de569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 HR policy documents\n",
      "1. employee_code_of_conduct.txt: 2595 characters\n",
      "2. leave_policy.txt: 2044 characters\n",
      "3. it_and_security_policy.txt: 1600 characters\n",
      "4. payroll_and_compensation_policy.txt: 1610 characters\n",
      "5. work_from_home_policy.txt: 1616 characters\n",
      "6. performance_review_policy.txt: 1453 characters\n"
     ]
    }
   ],
   "source": [
    "# Load the blog post\n",
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from pathlib import Path\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Define the path to your HR policies folder\n",
    "hr_policies_path = \"/home/anisha/Desktop/Projects/hrms_fullstack/uptiqai-hrm-system/Human-Resource-Management-System/rag/uptiq_hr_policies\"\n",
    "\n",
    "# Load all text files from the HR policies directory\n",
    "loader = DirectoryLoader(\n",
    "    hr_policies_path,\n",
    "    glob=\"*.txt\",  # Load all .txt files\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "\n",
    "# Load the documents\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(docs)} HR policy documents\")\n",
    "for i, doc in enumerate(docs):\n",
    "    filename = os.path.basename(doc.metadata['source'])\n",
    "    print(f\"{i+1}. {filename}: {len(doc.page_content)} characters\")\n",
    "\n",
    "    \n",
    "\n",
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200, \n",
    "    chunk_overlap=20\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index the chunks in a Chroma vector store\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=HuggingFaceEmbeddings(\n",
    "        model_name=\"all-MiniLM-L6-v2\"  # Fast and efficient model\n",
    "        # model_name=\"all-mpnet-base-v2\"  # Better quality but slower\n",
    "    ))\n",
    "\n",
    "# Create our retriever\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6529e4",
   "metadata": {},
   "source": [
    "Now, with our retriever ready, let’s explore our first query transformation technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa37693",
   "metadata": {},
   "source": [
    "<a id='part2-1'></a>\n",
    "## Multi-Query Generation\n",
    "\n",
    "A single user query represents just one perspective. Distance-based similarity search might miss relevant documents that use synonyms or discuss related concepts.\n",
    "\n",
    "The Multi-Query approach tackles this by using an LLM to generate several different versions of the user’s question, effectively searching from multiple angles.\n",
    "\n",
    "![Multi-Query Optimization (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*GjZoAISn6Jv3CBH87zUNPA.png)\n",
    "\n",
    "We’ll start by creating a prompt that instructs the LLM to generate these alternative questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57498a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Prompt for generating multiple queries\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "# Chain to generate the queries\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222c1244",
   "metadata": {},
   "source": [
    "Let’s test this chain and see what kind of queries it generates for our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "352fc1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 1. What is the leave policy at uptiq?  \n",
      "2. 2. How does uptiq handle employee leave?  \n",
      "3. 3. What time off policies does uptiq offer?  \n",
      "4. 4. What is the process for taking leave at uptiq?  \n",
      "5. 5. What types of leave are available at uptiq?\n"
     ]
    }
   ],
   "source": [
    "question = \"What are leave policies at uptiq?\"\n",
    "generated_queries_list = generate_queries.invoke({\"question\": question})\n",
    "\n",
    "# Print the generated queries\n",
    "for i, q in enumerate(generated_queries_list):\n",
    "    print(f\"{i+1}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984919ce",
   "metadata": {},
   "source": [
    "This is excellent. The LLM has rephrased our original question using different keywords like “break down complex tasks”, “methods”, and “process.” Now, we can retrieve documents for all of these queries and combine the results. A simple way to combine them is to take the unique set of all retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d23d057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique documents retrieved: 7\n"
     ]
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" A simple function to get the unique union of retrieved documents \"\"\"\n",
    "    # Flatten the list of lists and convert each Document to a string for uniqueness\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Build the retrieval chain\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "# Invoke the chain and check the number of documents retrieved\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "print(f\"Total unique documents retrieved: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0cba2",
   "metadata": {},
   "source": [
    "By searching with five different queries, we retrieved a total of 6 unique documents, likely capturing a more comprehensive set of information than a single query would have. Now we can feed this context into our final RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19ba58f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The leave policies at Uptiq are as follows:\n",
      "\n",
      "1. **Annual Leave**: \n",
      "   - Each employee is entitled to 18 paid annual leave days per calendar year.\n",
      "   - Applications must be made at least 5 days in advance, except in emergencies.\n",
      "   - A maximum of 6 unused days can be carried forward to the next year.\n",
      "\n",
      "2. **Sick Leave**: \n",
      "   - Employees receive 10 paid sick leave days annually.\n",
      "   - A medical certificate is required for absences exceeding 2 consecutive days.\n",
      "   - Unused sick leaves cannot be encashed or carried forward.\n",
      "\n",
      "3. **Casual Leave**: \n",
      "   - Employees are granted 7 casual leave days each year.\n",
      "   - Applications should be submitted at least 2 days in advance, except in emergencies.\n",
      "\n",
      "4. **Public Holidays**: \n",
      "   - Uptiq observes 12 company-declared public holidays each year, based on regional and national holiday calendars.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# The final RAG chain\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "# llm = ChatOpenAI(temperature=0)\n",
    "llm = ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(final_rag_chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce069080",
   "metadata": {},
   "source": [
    "> This answer is more robust because it’s based on a wider pool of relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabfa157",
   "metadata": {},
   "source": [
    "<a id='part2-2'></a>\n",
    "## RAG-Fusion\n",
    "\n",
    "Multi-Query is a great start, but simply taking a union of documents treats them all equally. What if one document was ranked highly by three of our queries, while another was a low-ranked result from only one?\n",
    "\n",
    "![RAG Fusion (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*qIJlH2bVjc1ZZflcniuHCw.png)\n",
    "\n",
    "The first is clearly more important. RAG-Fusion improves on Multi-Query by not just fetching documents, but also …\n",
    "\n",
    "> **re-ranking** them using a technique called **Reciprocal Rank Fusion (RRF)**.\n",
    "\n",
    "RRF intelligently combines results from multiple searches. It boosts the score of documents that appear consistently high across different result lists, pushing the most relevant content to the top.\n",
    "\n",
    "The code is very similar, but we’ll swap our `get_unique_union` function with an RRF implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c5475a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal Rank Fusion that intelligently combines multiple ranked lists \"\"\"\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # The core of RRF: documents ranked higher (lower rank value) get a larger score\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort documents by their new fused scores in descending order\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    return reranked_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdeaf42",
   "metadata": {},
   "source": [
    "The above function will re-rank the documents after they are fetched through similarity search, but we haven’t initialized it yet so let’s do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e180b3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total re-ranked documents retrieved: 13\n"
     ]
    }
   ],
   "source": [
    "# Use a slightly different prompt for RAG-Fusion\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Build the new retrieval chain with RRF\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "\n",
    "print(f\"Total re-ranked documents retrieved: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef3ed27",
   "metadata": {},
   "source": [
    "The final chain remains the same, but now it receives a more intelligently ranked context. RAG-Fusion is a powerful, low-effort way to increase the quality of your retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0aab1f",
   "metadata": {},
   "source": [
    "<a id='part2-3'></a>\n",
    "## Decomposition\n",
    "\n",
    "Some questions are too complex to be answered in a single step. For example, **“What are the main components of an LLM-powered agent, and how do they interact?”** This is really two questions in one.\n",
    "\n",
    "![Answer Recursively (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*oYttQUN_G0J_TZtigWjsGQ.png)\n",
    "\n",
    "The Decomposition technique uses an LLM to break down a complex query into a set of simpler, self-contained sub-questions. We can then answer each one and synthesize a final answer.\n",
    "\n",
    "We’ll start with a prompt designed for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c1c260c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1. What are the leave policies at uptiq, including types and amounts of leave provided?', '2. How does the performance review process work at uptiq, including frequency and criteria?', '3. Are there any additional policies or procedures related to leave and performance reviews at uptiq?']\n"
     ]
    }
   ],
   "source": [
    "# Decomposition prompt\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "# Chain to generate sub-questions\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Generate and print the sub-questions\n",
    "question = \"What are leave and performance review policies at uptiq?\"\n",
    "sub_questions = generate_queries_decomposition.invoke({\"question\": question})\n",
    "print(sub_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2693e4",
   "metadata": {},
   "source": [
    "The LLM successfully decomposed our complex question. Now, we can answer each of these individually and combine the results. One effective method is to answer each sub-question and use the resulting Q&A pairs as context to synthesize a final, comprehensive answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f3284577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Leave and Performance Review Policies at Uptiq**\n",
      "\n",
      "**Leave Policies:**\n",
      "\n",
      "1. **Annual Leave:**\n",
      "   - 18 days per year.\n",
      "   - Carry forward up to 6 days to the next year.\n",
      "\n",
      "2. **Sick Leave:**\n",
      "   - 10 days per year.\n",
      "   - Medical certificate required for absences exceeding 2 days.\n",
      "\n",
      "3. **Casual Leave:**\n",
      "   - 7 days per year.\n",
      "   - Requires 2 days' notice, except in emergencies.\n",
      "\n",
      "**Performance Review Process:**\n",
      "\n",
      "- **Frequency:**\n",
      "  - Conducted twice annually in June and December.\n",
      "  - Informal feedback sessions in March and September.\n",
      "\n",
      "- **Evaluation Criteria:**\n",
      "  - Technical skills.\n",
      "  - Productivity.\n",
      "  - Collaboration.\n",
      "  - Innovation.\n",
      "  - Alignment with company values.\n",
      "\n",
      "- **Feedback Method:**\n",
      "  - 360-degree approach, including input from peers, managers, and self-assessments.\n",
      "\n",
      "**Additional Policies:**\n",
      "- No additional policies or procedures related to leave and performance reviews are mentioned beyond the details provided above.\n"
     ]
    }
   ],
   "source": [
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# A list to hold the answers to our sub-questions\n",
    "rag_results = []\n",
    "for sub_question in sub_questions:\n",
    "    # Retrieve documents for each sub-question\n",
    "    retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
    "    \n",
    "    # Use our standard RAG chain to answer the sub-question\n",
    "    answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \"question\": sub_question})\n",
    "    rag_results.append(answer)\n",
    "\n",
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# Format the Q&A pairs into a single context string\n",
    "context = format_qa_pairs(sub_questions, rag_results)\n",
    "\n",
    "# Final synthesis prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the original question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(final_rag_chain.invoke({\"context\": context, \"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e769f2",
   "metadata": {},
   "source": [
    "By breaking the problem down, we constructed a much more detailed and structured answer than we would have otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81e22a",
   "metadata": {},
   "source": [
    "<a id='part2-4'></a>\n",
    "## Step-Back Prompting\n",
    "\n",
    "Sometimes, a user’s query is too specific, while our documents contain the more general, underlying information needed to answer it.\n",
    "\n",
    "![Step Back Prompting (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:875/1*6lrhGv1fdcmLKMVu5tU3uQ.png)\n",
    "\n",
    "> For example, a user might ask, “Could the members of The Police perform lawful arrests?”\n",
    "\n",
    "A direct search for this might fail. The Step-Back technique uses an LLM to take a “step back” and form a more general question, like “What are the powers and duties of the band The Police?” We then retrieve context for *both* the specific and general questions, providing a richer context for the final answer.\n",
    "\n",
    "We can teach the LLM this pattern using few-shot examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1825be60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# Few-shot examples to teach the model how to generate step-back (more generic) questions \n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Can I carry forward 8 unused annual leave days into the next year?\",\n",
    "        \"output\": \"What is Uptiq’s policy on carrying forward unused annual leave?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Do I get reimbursed if I buy my own Wi-Fi router while working from home?\",\n",
    "        \"output\": \"What expenses are reimbursed under Uptiq’s Work From Home policy?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Define how each example is formatted in the prompt\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),  # User input\n",
    "    (\"ai\", \"{output}\")     # Model's response\n",
    "])\n",
    "\n",
    "# Wrap the few-shot examples into a reusable prompt template\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# Full prompt includes system instruction, few-shot examples, and the user question\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"You are an AI assistant trained on HR policies of Uptiq. Your task is to step back and paraphrase a question \"\n",
    "     \"to a more generic step-back question, which is easier to answer. Here are a few examples:\"),\n",
    "    few_shot_prompt,\n",
    "    (\"user\", \"{question}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ea14b",
   "metadata": {},
   "source": [
    "Now, we can simply define the chain for step back approach, so let’s do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7b3610f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Question: What are leave and performance review policies at uptiq?\n",
      "Step-Back Question: What are Uptiq’s leave policies?  \n",
      "What is Uptiq’s performance review policy?\n"
     ]
    }
   ],
   "source": [
    "# Define a chain to generate step-back questions using the prompt and an OpenAI model\n",
    "generate_queries_step_back = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run the chain on a specific question\n",
    "question = \"What are leave and performance review policies at uptiq?\"\n",
    "step_back_question = generate_queries_step_back.invoke({\"question\": question})\n",
    "\n",
    "# Output the original and generated step-back question\n",
    "print(f\"Original Question: {question}\")\n",
    "print(f\"Step-Back Question: {step_back_question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a3c61",
   "metadata": {},
   "source": [
    "This is an important step-back question. It broadens the scope to general software engineering, which will likely pull in foundational documents that can then be combined with the specific context about LLM agents. Now we can build a chain that uses both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7e13c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Prompt for the final response\n",
    "response_prompt_template = \"\"\"You are an AI assistant trained on HR policies of Uptiq. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# Normal Context\n",
    "{normal_context}\n",
    "\n",
    "# Step-Back Context\n",
    "{step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "# The full chain\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the original question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7b8dfe",
   "metadata": {},
   "source": [
    "This is the output we get, when we run this step back prompt chain with our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c551aa50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Uptiq's HR Policies: Leave and Performance Review**\n",
      "\n",
      "**Performance Review Policy:**\n",
      "- **Frequency:** Conducted twice annually in June and December, with mid-year informal feedback sessions in March and September.\n",
      "- **Evaluation Criteria:** Assesses technical skills, productivity, collaboration, innovation, and alignment with company values.\n",
      "- **Feedback Process:** Utilizes 360-degree feedback, including peer, manager, and self-assessment, with anonymous peer feedback for fairness.\n",
      "\n",
      "**Leave Policy:**\n",
      "- **Introduction:** Uptiq emphasizes work-life balance by offering a fair and flexible leave policy to ensure employee well-being. Specific details on types of leave and related policies are not provided in the available context.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83694b96",
   "metadata": {},
   "source": [
    "<a id='part2-5'></a>\n",
    "## HyDE\n",
    "\n",
    "This final technique is one of the most clever. The core problem of retrieval is that a user’s query might use different words than the document (the “vocabulary mismatch” problem).\n",
    "\n",
    "![HyDE (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*YQVJMOpDBU6l54atHoFpJg.png)\n",
    "\n",
    "**HyDE (Hypothetical Document Embeddings)** proposes a radical solution: First, have an LLM generate a *hypothetical* answer to the question. This fake document, while not factually correct, will be semantically rich and use the kind of language we expect to find in a real answer.\n",
    "\n",
    "We then embed this hypothetical document and use its embedding to perform the retrieval. The result is that we find real documents that are semantically very similar to an ideal answer.\n",
    "\n",
    "Let’s start by creating a prompt to generate this hypothetical document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98adfc6b-1ae7-4d44-9257-ae08fab69b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Do I get reimbursed for electricity bills if I work from home?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e023ddf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine whether you can be reimbursed for electricity bills while working from home, consider the following factors:\n",
      "\n",
      "1. **Employment Status**: \n",
      "   - **Full-time Remote Employees**: Some companies may offer stipends or reimbursements for home office expenses, including utilities. This can vary by company policy.\n",
      "   - **Freelancers/Independent Contractors**: Typically, these individuals handle their own expenses, but they may be able to deduct a portion of their utility bills as business expenses on their taxes.\n",
      "\n",
      "2. **Tax Implications**: \n",
      "   - In some countries, a portion of home utility bills can be deducted as business expenses on tax returns. This is not reimbursement from an employer but a tax deduction.\n",
      "\n",
      "3. **Company Policies**: \n",
      "   - Reimbursement policies vary by employer. Some companies provide monthly stipends, while others require documentation. This is more common in sectors like tech.\n",
      "\n",
      "4. **Calculation of Expenses**: \n",
      "   - Determine the work-related portion of electricity usage, possibly by estimating based on home office space or equipment used. This might involve calculating square footage or average usage during work hours.\n",
      "\n",
      "5. **Legal and Government Guidelines**: \n",
      "   - Check for any legal requirements or government resources that outline reimbursable expenses for remote work.\n",
      "\n",
      "In conclusion, reimbursement for electricity bills while working from home depends on your employment status, company policies, tax laws, and how expenses are calculated. It's advisable to consult with your employer or a tax professional for specific guidance.\n"
     ]
    }
   ],
   "source": [
    "# HyDE prompt\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Chain to generate the hypothetical document\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Generate and print the hypothetical document\n",
    "hypothetical_document = generate_docs_for_retrieval.invoke({\"question\": question})\n",
    "print(hypothetical_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57de5e21",
   "metadata": {},
   "source": [
    "This passage is a perfect, textbook-style answer. Now, we use its embedding to find real documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e77b8fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documents, the company does not offer reimbursement for electricity bills when working from home. The policy only covers internet charges up to INR 1,500 per month with valid bills. For further clarification, it's recommended to consult with HR or review the company's policy manual. \n",
      "\n",
      "**Answer:** No, the company does not reimburse electricity bills for work-from-home arrangements. Reimbursement is only provided for internet charges up to INR 1,500 per month with valid bills.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve documents using the HyDE approach\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retrieved_docs = retrieval_chain.invoke({\"question\": question})\n",
    "\n",
    "# Use our standard RAG chain to generate the final answer from the retrieved context\n",
    "response = final_rag_chain.invoke({\"context\": retrieved_docs, \"question\": question})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19affa43",
   "metadata": {},
   "source": [
    "By using a hypothetical document as a **lure**, HyDE helped us zero in on the most relevant chunks in our knowledge base, demonstrating another powerful tool in our RAG toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9237d2ab",
   "metadata": {},
   "source": [
    "<a id='part3'></a>\n",
    "# Routing & Query Construction\n",
    "\n",
    "Our RAG system is getting smarter, but in a real-world scenario, knowledge isn’t stored in a single, uniform library.\n",
    "\n",
    "> We often have multiple data sources: documentation for different programming languages, internal wikis, public websites, or databases with structured metadata.\n",
    "\n",
    "![Routing and Query Transformation (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*cost0_AWB8NKp0WxZlH7fA.png)\n",
    "\n",
    "Sending every query to every source is wildly inefficient and can lead to noisy, irrelevant results.\n",
    "\n",
    "This is where our RAG system needs to evolve from a simple librarian into an **intelligent switchboard operator**. It needs the ability to first *analyze* an incoming query and then *route* it to the correct destination or *construct* a more precise, structured query for retrieval. This section dives into the techniques that make this possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e24fb2",
   "metadata": {},
   "source": [
    "<a id='part3-1'></a>\n",
    "## Logical Routing\n",
    "\n",
    "Routing is a classification problem. Given a user’s question, we need to classify it into one of several predefined categories. While traditional ML models can do this, we can leverage the powerful reasoning engine we already have: the LLM itself.\n",
    "\n",
    "![Logical Routing (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:875/1*PK9xKW0o-72xmmLaAAozeA.png)\n",
    "\n",
    "By providing the LLM with a clear schema (a set of possible categories), we can ask it to make the classification decision for us.\n",
    "\n",
    "We’ll start by defining the “contract” for our LLM’s output using a Pydantic model. This schema explicitly tells the LLM the possible destinations for a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d53c165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Literal\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# # Define the data model for our router's output\n",
    "# class RouteQuery(BaseModel):\n",
    "#     \"\"\"A data model to route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "#     # The 'datasource' field must be one of the three specified literal strings.\n",
    "#     # This enforces a strict set of choices for the LLM.\n",
    "#     datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "#         ...,  # The '...' indicates that this field is required.\n",
    "#         description=\"Given a user question, choose which datasource would be most relevant for answering their question.\",\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cff907f3-bced-495e-8de3-8470ff74cf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anisha/Desktop/Projects/hrms_fullstack/uptiqai-hrm-system/Human-Resource-Management-System/rag/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import Literal\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Define the data model for routing based on HR policy files\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"A data model to route a user query to the most relevant HR policy file.\"\"\"\n",
    "\n",
    "    # The 'file_name' field must be one of the HR policy files.\n",
    "    file_name: Literal[\n",
    "        \"employee_code_of_conduct.txt\",\n",
    "        \"leave_policy.txt\",\n",
    "        \"work_from_home_policy.txt\",\n",
    "        \"payroll_and_compensation_policy.txt\",\n",
    "        \"performance_review_policy.txt\",\n",
    "        \"it_and_security_policy.txt\"\n",
    "    ] = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Given a user question, choose which HR policy file \"\n",
    "            \"would be most relevant for answering their question.\"\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1fab2c",
   "metadata": {},
   "source": [
    "With our schema defined, we can now build the router chain. We’ll use a prompt to give the LLM its instructions and then use the `.with_structured_output()` method to ensure its response perfectly matches our `RouteQuery` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b663642",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize our LLM\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "\n",
    "# Create a new LLM instance that is \"structured\" to output our Pydantic model\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# The system prompt provides the core instruction for the LLM's task.\n",
    "# system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "# Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "Given a user question, choose which HR policy file would be most relevant for answering their question.\"\"\"\n",
    "\n",
    "# The full prompt template combines the system message and the user's question.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the complete router chain\n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae15a4",
   "metadata": {},
   "source": [
    "Now, let’s test our router. We’ll pass it a question that is clearly about Python and inspect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2bef3ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Question: Can I carry forward my annual leave?\n",
      "Routed to File: leave_policy.txt\n",
      "\n",
      "User Question: What should I do if my company laptop gets stolen?\n",
      "Routed to File: it_and_security_policy.txt\n",
      "\n",
      "User Question: Am I allowed to wear jeans for a client meeting?\n",
      "Routed to File: employee_code_of_conduct.txt\n",
      "\n",
      "User Question: How often does Uptiq conduct performance reviews?\n",
      "Routed to File: performance_review_policy.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example user queries\n",
    "questions = [\n",
    "    \"Can I carry forward my annual leave?\",                         # leave_policy.txt\n",
    "    \"What should I do if my company laptop gets stolen?\",            # it_and_security_policy.txt\n",
    "    \"Am I allowed to wear jeans for a client meeting?\",              # employee_code_of_conduct.txt\n",
    "    \"How often does Uptiq conduct performance reviews?\"              # performance_review_policy.txt\n",
    "]\n",
    "\n",
    "# Invoke the router for each query and print results\n",
    "for q in questions:\n",
    "    result = router.invoke({\"question\": q})\n",
    "    print(f\"User Question: {q}\")\n",
    "    print(f\"Routed to File: {result.file_name}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e877b70f",
   "metadata": {},
   "source": [
    "The output is an instance of our `RouteQuery` model, and the LLM has correctly identified `python_docs` as the appropriate datasource. This structured output is now something we can reliably use in our code to implement branching logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5110a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain for python_docs\n"
     ]
    }
   ],
   "source": [
    "# from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# def choose_route(result):\n",
    "#     \"\"\"A function to determine the downstream logic based on the router's output.\"\"\"\n",
    "#     if \"python_docs\" in result.datasource.lower():\n",
    "#         # In a real app, this would be a complete RAG chain for Python docs\n",
    "#         return \"chain for python_docs\"\n",
    "#     elif \"js_docs\" in result.datasource.lower():\n",
    "#         # This would be the chain for JavaScript docs\n",
    "#         return \"chain for js_docs\"\n",
    "#     else:\n",
    "#         # And this for Go docs\n",
    "#         return \"chain for golang_docs\"\n",
    "\n",
    "# # The full chain now includes the routing and branching logic\n",
    "# full_chain = router | RunnableLambda(choose_route)\n",
    "\n",
    "# # Let's run the full chain\n",
    "# final_destination = full_chain.invoke({\"question\": question})\n",
    "\n",
    "# print(final_destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbd91ba",
   "metadata": {},
   "source": [
    "Our switchboard correctly routed the Python-related query. This approach is incredibly powerful for building multi-source RAG systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e018b4",
   "metadata": {},
   "source": [
    "<a id='part3-2'></a>\n",
    "## Semantic Routing\n",
    "\n",
    "Logical routing works perfectly when you have clearly defined categories. But what if you want to route based on the *style* or *domain* of a question? For example, you might want to answer physics questions with a serious, academic tone and math questions with a step-by-step, pedagogical approach. This is where **Semantic Routing** comes in.\n",
    "\n",
    "![Semantic Routing (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*mzz-ncmrzdwQU37GFgPeTw.png)\n",
    "\n",
    "> Instead of classifying the query, we define multiple expert prompts.\n",
    "\n",
    "We then embed the user’s query and each of our prompt templates, and use cosine similarity to find the prompt that is most semantically aligned with the query.\n",
    "\n",
    "First, let’s define our two expert personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d81b5b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# A prompt for a hr expert\n",
    "hr_template = \"\"\"You are an HR policies assistant for the company Uptiq.\n",
    "    You answer questions about leave policies, payroll, employee benefits, and workplace compliance.\n",
    "    Always explain clearly and reference HR rules.\n",
    "\n",
    "    Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "# A prompt for a it expert\n",
    "it_template = \"\"\"You are an IT helpdesk expert.\n",
    "    You answer questions related to technical troubleshooting, software, hardware,\n",
    "    network issues, and security guidelines in a simple and actionable way.\n",
    "\n",
    "    Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "\n",
    "# A prompt for a law expert\n",
    "law_template = \"\"\"You are a legal advisor. \n",
    "    You explain laws, regulations, workplace compliance, contracts, and employee rights \n",
    "    in a clear and simple manner. \n",
    "    Always include a disclaimer that this is not professional legal advice.\n",
    "\n",
    "    Here is a question:\n",
    "{query}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919b96fa",
   "metadata": {},
   "source": [
    "Now, we’ll create the routing function that performs the embedding and similarity comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b74e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize the embedding model\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"all-MiniLM-L6-v2\"  # Fast and efficient model\n",
    "        # model_name=\"all-mpnet-base-v2\"  # Better quality but slower\n",
    "    )\n",
    "\n",
    "\n",
    "# Store our templates and their embeddings for comparison\n",
    "prompt_templates = [hr_template, it_template, law_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "def prompt_router(input):\n",
    "    \"\"\"A function to route the input query to the most similar prompt template.\"\"\"\n",
    "    # 1. Embed the incoming user query\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    \n",
    "    # 2. Compute the cosine similarity between the query and all prompt templates\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    \n",
    "    # 3. Find the index of the most similar prompt\n",
    "    most_similar_index = similarity.argmax()\n",
    "\n",
    "    template_names = [\"hr_template\", \"law_template\", \"it_template\"]\n",
    "    choosen_name = template_names[most_similar_index]\n",
    "    \n",
    "    # 4. Select the most similar prompt template\n",
    "    chosen_prompt = prompt_templates[most_similar_index]\n",
    "    \n",
    "    print(f\"DEBUG: Routed to {choosen_name} template.\")\n",
    "    \n",
    "    # 5. Return the chosen prompt object\n",
    "    return PromptTemplate.from_template(chosen_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890796f3",
   "metadata": {},
   "source": [
    "With the routing logic in place, we can build the full chain that dynamically selects the right expert for the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6947fcf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Routed to it_template template.\n",
      "If your company laptop is stolen, follow these organized steps to address the situation effectively:\n",
      "\n",
      "1. **Report the Theft:**\n",
      "   - Inform your supervisor or IT department immediately to notify them of the theft.\n",
      "   - File a police report, as this may be required for insurance purposes and could aid in recovery.\n",
      "\n",
      "2. **Secure Data and Accounts:**\n",
      "   - Check with IT to confirm if the laptop was encrypted for data protection.\n",
      "   - Change passwords for all accounts accessed on the laptop to prevent unauthorized access.\n",
      "\n",
      "3. **Assess Data and Inform Stakeholders:**\n",
      "   - Determine if sensitive company or client data was on the laptop and inform your superiors.\n",
      "   - Allow the company to assess potential data breaches and follow their protocols.\n",
      "\n",
      "4. **Utilize Tracking Software:**\n",
      "   - Check with IT if the laptop has tracking software (e.g., Find My Mac, LoJack) to potentially locate it or remotely wipe data.\n",
      "\n",
      "5. **Insurance and Replacement:**\n",
      "   - Inquire about company insurance coverage for stolen laptops and the process for obtaining a replacement.\n",
      "\n",
      "6. **Document Everything:**\n",
      "   - Keep detailed records of the theft, reporting steps, and subsequent actions for potential investigations or reviews.\n",
      "\n",
      "7. **Personal Data Backup:**\n",
      "   - Check if personal files were backed up elsewhere to prevent data loss.\n",
      "\n",
      "8. **Review Security Practices:**\n",
      "   - Consider using a cable lock and being more vigilant about laptop security in the future.\n",
      "\n",
      "9. **Client and Third-Party Communication:**\n",
      "   - Allow the company to handle notifications to clients or third parties regarding potential data risks.\n",
      "\n",
      "By following these steps, you can help protect company data, facilitate recovery, and ensure proper procedures are followed.\n"
     ]
    }
   ],
   "source": [
    "# The final chain that combines the router with the LLM\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)  # Dynamically select the prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "pip freeze > requirements-dev.txt\n",
    "# Ask a physics question\n",
    "print(chain.invoke(\"What should I do if my company laptop gets stolen?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18bd79d",
   "metadata": {},
   "source": [
    "Perfect. The router correctly identified the question as physics-related and used the physics professor prompt, resulting in a concise and accurate answer. This technique is excellent for creating specialized agents that adapt their persona to the user’s needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "50a42027-bd92-4a68-a2dc-c42b65abc90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements-dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "488ce771-e81d-4b49-9a19-80476b706731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip-chill\n",
      "  Downloading pip_chill-1.0.3-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Downloading pip_chill-1.0.3-py2.py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: pip-chill\n",
      "Successfully installed pip-chill-1.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pip-chill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f12d8133-cbdf-4d68-bb2a-235aa9b3cd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/anisha/Desktop/Projects/hrms_fullstack/uptiqai-hrm-system/Human-Resource-Management-System/rag/myenv/lib/python3.12/site-packages/pip_chill/pip_chill.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "!pip-chill > requirements.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
