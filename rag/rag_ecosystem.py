# -*- coding: utf-8 -*-
"""rag_ecosystem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16vuSMDDomRraTd-zO20niAsK9i_o2y6G

# RAG Ecosystem

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-3100/) [![LangChain](https://img.shields.io/badge/LangChain-%23007ACC.svg?logo=LangChain)](https://www.langchain.com/) [![DeepEval](https://img.shields.io/badge/DeepEval-Evaluation-orange)](https://github.com/confident-ai/deepeval) [![RAGAS](https://img.shields.io/badge/RAGAS-Evaluation-blueviolet)](https://github.com/explodinggradients/ragas) [![OpenAI](https://img.shields.io/badge/OpenAI-API-lightgrey)](https://openai.com/) [![Cohere](https://img.shields.io/badge/Cohere-API-yellowgreen)](https://cohere.com/) [![Medium](https://img.shields.io/badge/Medium-Blog-black?logo=medium)](https://medium.com/@fareedkhandev/8f23349b96a4)

Creating an entire RAG based AI system depends on many different components with each requires it’s own optimization and careful implementation. These components includes:

![Production Ready RAG System (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:2400/1*ZjozYulECfqrzgMaTEZ-Rg.png)

- **Query Transformations:** Rewriting user questions to be more effective for retrieval.
- **Intelligent Routing:** Directing a query to the correct data source or a specialized tool.
- **Indexing:** Creating a multi-layered knowledge base.
- **Retrieval and Re-ranking:** Filtering noise and prioritizing the most relevant context.
- **Self-Correcting Agentic Flows:** Building systems that can grade and improve their own work.
- **End-to-End Evaluation:** Objectively measuring the performance of the entire pipeline.

and much more …

> We will learn and code each part of the RAG ecosystem along with visuals for easier understanding, starting from the basics to advanced techniques.

All the code (Theory + Notebook) is available in my GitHub Repo:

[[[[[[[[[[[[[[ LINK ]]]]]]]]]]]]

## Table of Contents

- [Understanding Basic RAG System](#part1)
  - [Indexing Phase](#part1-1)
  - [Retrieval](#part1-2)
  - [Generation](#part1-3)
- [Advanced Query Transformations](#part2)
  - [Multi-Query Generation](#part2-1)
  - [RAG-Fusion](#part2-2)
  - [Decomposition](#part2-3)
  - [Step-Back Prompting](#part2-4)
  - [HyDE](#part2-5)
- [Routing & Query Construction](#part3)
  - [Logical Routing](#part3-1)
  - [Semantic Routing](#part3-2)
  - [Query Structuring](#part3-3)
- [Advanced Indexing Strategies](#part4)
  - [Multi-Representation Indexing](#part4-1)
  - [Hierarchical Indexing (RAPTOR) Knowledge Tree](#part4-2)
  - [Token-Level Precision (ColBERT)](#part4-3)
- [Advanced Retrieval & Generation](#part5)
  - [Dedicated Re-ranking](#part5-1)
  - [Self-Correction using AI Agents](#part5-2)
  - [Impact of Long Context](#part5-3)
- [Manual RAG Evaluation](#part6)
  - [The Core Metrics: What Should We Measure?](#part6-1)
  - [Building Evaluators from Scratch with LangChain](#part6-2)
- [Evaluation with Frameworks](#part7)
  - [Rapid Evaluation with deepeval](#part7-1)
  - [Another Powerful Alternative with grouse](#part7-2)
  - [Evaluation with RAGAS](#part7-3)
- [Summarizing Everything](#part8)

<a id='part1'></a>
# Understanding Basic RAG System

Before we look into the basics of RAG, let’s install core Python libraries commonly used for AI products, such as LangChain and others.
"""

# Installing Required Modules
# !pip install langchain langchain_community langchain-openai langchainhub chromadb tiktoken

"""We can now simply set the environment variables for tracing and other tasks, such as the LLMs API provider we will be using."""

import os

# Set LangChain API endpoint and API key for tracing with LangSmith
# os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'
# os.environ['LANGCHAIN_API_KEY'] = '<your-api-key>'  # Replace with your LangChain API key

# Set OpenAI API key for using OpenAI models
# os.environ['OPENAI_API_KEY'] = '<your-api-key>'  # Replace with your OpenAI API key
os.environ['GEMINI_API_KEY'] = 'AIzaSyANRVAIhB209L-LUL0pAVulj-BhI7roHBw'

"""You can obtain your `LangSmith` API key from [their official documentation](https://www.langchain.com/langsmith) to trace our RAG product throughout this blog. For the LLM, we will be using the `OpenAI` API but as you may already know, `LangChain` supports a variety of LLM providers as well.

The core RAG pipeline is the foundation of any advanced system, and understanding its components is important. Therefore, before going into the details of advanced components, we first need to understand the core logic of how a RAG system works, **but you can skip this section if you are already aware of how RAG system works.**

![Basic RAG system (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*c_yxo0cUH8u7o5an-Tzi0g.png)

This simplest RAG can be break into three components:

- **Indexing**: Organize and store data in a structured format to enable efficient searching.
- **Retrieval**: Search and fetch relevant data based on a query or input.
- **Generation**: Create a final response or output using the retrieved data.

Let’s build this simple pipeline from the ground up to see how each piece works.

<a id='part1-1'></a>
## Indexing Phase

Before our RAG system can answer any questions, it needs knowledge to draw from. For this, we’ll use a `WebBaseLoader` to pull content directly from [Lilian Weng's excellent blog post](https://lilianweng.github.io/posts/2023-06-23-agent/) on LLM-powered agents.

![Indexing phase (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:875/1*dnSg_QmGd4J030_bznvUPw.png)
"""

import os
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from pathlib import Path

# Define the path to your HR policies folder
hr_policies_path = "/home/anisha/Desktop/Projects/hrms_fullstack/uptiqai-hrm-system/Human-Resource-Management-System/rag/uptiq_hr_policies"

# Load all text files from the HR policies directory
loader = DirectoryLoader(
    hr_policies_path,
    glob="*.txt",  # Load all .txt files
    loader_cls=TextLoader,
    loader_kwargs={"encoding": "utf-8"}
)

# Load the documents
docs = loader.load()

print(f"Loaded {len(docs)} HR policy documents")
for i, doc in enumerate(docs):
    filename = os.path.basename(doc.metadata['source'])
    print(f"{i+1}. {filename}: {len(doc.page_content)} characters")

"""The `bs_kwargs` argument helps us target only the relevant HTML tags (`post-content`, `post-title`, etc.), cleaning up our data from the start.

Now that we have the document, we face our first challenge. Feeding a massive document directly into an LLM is inefficient and often impossible due to context window limits.

> This is why **chunking** is a critical step. We need to break the document into smaller, semantically meaningful pieces.

The `RecursiveCharacterTextSplitter` is the recommended tool for this job because it intelligently tries to keep paragraphs and sentences intact.
"""

from langchain.text_splitter import RecursiveCharacterTextSplitter

# Create a text splitter to divide text into chunks of 1000 (200) characters with 200-character (20) overlap
text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)

# Split the loaded documents into smaller chunks
splits = text_splitter.split_documents(docs)

"""With `chunk_size=1000`, we are creating chunks of 1000 characters, and `chunk_overlap=200` ensures there is some continuity between them, which helps preserve context.

Our text is now split, but it’s still just text. To perform similarity searches, we need to convert these chunks into numerical representations called **embeddings**. We will then store these embeddings in a **vector store**, which is a specialized database designed for efficient searching of vectors.

The `Chroma` vector store and `OpenAIEmbeddings` make this incredibly simple. The following line handles both embedding and indexing in one go.
"""

# pip install langchain_google_genai

from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

# Embed the text chunks and store them in a Chroma vector store for similarity search
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=OpenAIEmbeddings()  # Use OpenAI's embedding model to convert text into vectors
)

# pip install langchain_huggingface

# pip install sentence-transformers

from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings

# Using sentence-transformers model (runs locally, completely free)
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=HuggingFaceEmbeddings(
        model_name="all-MiniLM-L6-v2"  # Fast and efficient model
        # model_name="all-mpnet-base-v2"  # Better quality but slower
    )
)

"""With our knowledge indexed, we are now ready to start asking questions.

<a id='part1-2'></a>
## Retrieval

The vector store is our library, and the **retriever** is our smart librarian. It takes a user’s query, embeds it, and then fetches the most semantically similar chunks from the vector store.

![Retrieval Phase (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*jtf1FoBGfpnDPTTu9N94Wg.png)
"""

# Create a retriever from the vector store
retriever = vectorstore.as_retriever()

"""Let’s test it. We’ll ask a question and see what our retriever finds."""

# Retrieve relevant documents for a query
docs = retriever.get_relevant_documents("What is Salary Structure?")

# Print the content of the first retrieved document
print(docs[0].page_content)

"""As you can see, the retriever successfully pulled the most relevant chunk from the blog post that directly discusses “Task decomposition.” This piece of context is exactly what the LLM needs to form an accurate answer.

<a id='part1-3'></a>
## Generation

We have our context, but we need an LLM to read it and formulate a human-friendly answer. This is the **“Generation”** step in RAG.

![Generation Step (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*0K6ognTAEOJQmb6KDL9wBw.png)

First, we need a good prompt template. This instructs the LLM on how to behave. Instead of writing our own, we can pull a pre-optimized one from LangChain Hub.
"""

from langchain import hub

# Pull a pre-made RAG prompt from LangChain Hub
prompt = hub.pull("rlm/rag-prompt")

# printing the prompt
print(prompt)

"""Next, we initialize our LLM. We’ll use `gpt-3.5-turbo`."""

# from langchain_openai import ChatOpenAI

# # Initialize the LLM
# llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain-groq

import getpass
import os

if "GROQ_API_KEY" not in os.environ:
    os.environ["GROQ_API_KEY"] = "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"

from langchain_groq import ChatGroq

llm = ChatGroq(
    model="deepseek-r1-distill-llama-70b",
    temperature=0,
    max_tokens=None,
    reasoning_format="parsed",
    timeout=None,
    max_retries=2
)

"""Now for the final step: chaining everything together. Using the LangChain Expression Language (LCEL), we can pipe the output of one component into the input of the next."""

from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# Helper function to format retrieved documents
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Define the full RAG chain
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

"""Let’s break down this chain:

1. `{"context": retriever | format_docs, "question": RunnablePassthrough()}`: This part runs in parallel. It sends the user's question to the `retriever` to get documents, which are then formatted into a single string by `format_docs`. Simultaneously, `RunnablePassthrough` passes the original question through unchanged.
2. `| prompt`: The context and question are fed into our prompt template.
3. `| llm`: The formatted prompt is sent to the LLM.
4. `| StrOutputParser()`: This cleans up the LLM's output into a simple string.

Now, let’s invoke the entire chain.
"""

# Ask a question using the RAG chain
response = rag_chain.invoke("What is Salary Structure?")
print(response)

"""And there we have it, our RAG pipeline successfully retrieved relevant information about **“Task Decomposition”** and used it to generate a concise, accurate answer. This simple chain forms the foundation upon which we will build more advanced and powerful capabilities.

<a id='part2'></a>
# Advanced Query Transformations

So, now that we understand the fundamentals of RAG pipeline. But production systems often reveal the limitations of this basic approach. One of the most common failure points is the user’s query itself.

![Query Transformation (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*FO2U9QA49kjn6OaBGZuq8A.png)

> A query might be too specific, too broad, or use different vocabulary than our source documents, leading to poor retrieval results.

The solution isn’t to blame the user, it’s to make our system smarter. **Query Transformation** is a set of powerful techniques designed to re-write, expand, or break down the original question to significantly improve retrieval accuracy.

Instead of relying on a single query, we’ll engineer multiple, better-informed queries to cast a wider and more accurate net.

To test these new techniques, we will use the same indexed knowledge base from Basic RAG pipeline section that we have just gone through previously. This ensures we can directly compare the results and see the improvements.

As a quick refresher, here’s how we set up our retriever:
"""

# Load the blog post
import os
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from pathlib import Path
from langchain_huggingface import HuggingFaceEmbeddings

# Define the path to your HR policies folder
hr_policies_path = "/home/anisha/Desktop/Projects/hrms_fullstack/uptiqai-hrm-system/Human-Resource-Management-System/rag/uptiq_hr_policies"

# Load all text files from the HR policies directory
loader = DirectoryLoader(
    hr_policies_path,
    glob="*.txt",  # Load all .txt files
    loader_cls=TextLoader,
    loader_kwargs={"encoding": "utf-8"}
)

# Load the documents
docs = loader.load()

print(f"Loaded {len(docs)} HR policy documents")
for i, doc in enumerate(docs):
    filename = os.path.basename(doc.metadata['source'])
    print(f"{i+1}. {filename}: {len(doc.page_content)} characters")



# Split the documents into chunks
text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
    chunk_size=200,
    chunk_overlap=20
)
splits = text_splitter.split_documents(docs)

# Index the chunks in a Chroma vector store
vectorstore = Chroma.from_documents(documents=splits,
                                    embedding=HuggingFaceEmbeddings(
        model_name="all-MiniLM-L6-v2"  # Fast and efficient model
        # model_name="all-mpnet-base-v2"  # Better quality but slower
    ))

# Create our retriever
retriever = vectorstore.as_retriever()

"""Now, with our retriever ready, let’s explore our first query transformation technique.

<a id='part2-1'></a>
## Multi-Query Generation

A single user query represents just one perspective. Distance-based similarity search might miss relevant documents that use synonyms or discuss related concepts.

The Multi-Query approach tackles this by using an LLM to generate several different versions of the user’s question, effectively searching from multiple angles.

![Multi-Query Optimization (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*GjZoAISn6Jv3CBH87zUNPA.png)

We’ll start by creating a prompt that instructs the LLM to generate these alternative questions.
"""

from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq

# Prompt for generating multiple queries
template = """You are an AI language model assistant. Your task is to generate five
different versions of the given user question to retrieve relevant documents from a vector
database. By generating multiple perspectives on the user question, your goal is to help
the user overcome some of the limitations of the distance-based similarity search.
Provide these alternative questions separated by newlines. Original question: {question}"""
prompt_perspectives = ChatPromptTemplate.from_template(template)


llm = ChatGroq(
    model="deepseek-r1-distill-llama-70b",
    temperature=0,
    max_tokens=None,
    reasoning_format="parsed",
    timeout=None,
    max_retries=2
)

# Chain to generate the queries
generate_queries = (
    prompt_perspectives
    | llm
    | StrOutputParser()
    | (lambda x: x.split("\n"))
)

"""Let’s test this chain and see what kind of queries it generates for our question."""

question = "What are leave policies at uptiq?"
generated_queries_list = generate_queries.invoke({"question": question})

# Print the generated queries
for i, q in enumerate(generated_queries_list):
    print(f"{i+1}. {q}")

"""This is excellent. The LLM has rephrased our original question using different keywords like “break down complex tasks”, “methods”, and “process.” Now, we can retrieve documents for all of these queries and combine the results. A simple way to combine them is to take the unique set of all retrieved documents."""

from langchain.load import dumps, loads

def get_unique_union(documents: list[list]):
    """ A simple function to get the unique union of retrieved documents """
    # Flatten the list of lists and convert each Document to a string for uniqueness
    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]
    unique_docs = list(set(flattened_docs))
    return [loads(doc) for doc in unique_docs]

# Build the retrieval chain
retrieval_chain = generate_queries | retriever.map() | get_unique_union

# Invoke the chain and check the number of documents retrieved
docs = retrieval_chain.invoke({"question": question})
print(f"Total unique documents retrieved: {len(docs)}")

"""By searching with five different queries, we retrieved a total of 6 unique documents, likely capturing a more comprehensive set of information than a single query would have. Now we can feed this context into our final RAG chain."""

from operator import itemgetter
from langchain_core.runnables import RunnablePassthrough

# The final RAG chain
template = """Answer the following question based on this context:

{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)
# llm = ChatOpenAI(temperature=0)
llm = ChatGroq(
    model="deepseek-r1-distill-llama-70b",
    temperature=0,
    max_tokens=None,
    reasoning_format="parsed",
    timeout=None,
    max_retries=2
)

final_rag_chain = (
    {"context": retrieval_chain, "question": itemgetter("question")}
    | prompt
    | llm
    | StrOutputParser()
)

print(final_rag_chain.invoke({"question": question}))

"""> This answer is more robust because it’s based on a wider pool of relevant documents.

<a id='part2-2'></a>
## RAG-Fusion

Multi-Query is a great start, but simply taking a union of documents treats them all equally. What if one document was ranked highly by three of our queries, while another was a low-ranked result from only one?

![RAG Fusion (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*qIJlH2bVjc1ZZflcniuHCw.png)

The first is clearly more important. RAG-Fusion improves on Multi-Query by not just fetching documents, but also …

> **re-ranking** them using a technique called **Reciprocal Rank Fusion (RRF)**.

RRF intelligently combines results from multiple searches. It boosts the score of documents that appear consistently high across different result lists, pushing the most relevant content to the top.

The code is very similar, but we’ll swap our `get_unique_union` function with an RRF implementation.
"""

def reciprocal_rank_fusion(results: list[list], k=60):
    """ Reciprocal Rank Fusion that intelligently combines multiple ranked lists """
    fused_scores = {}

    # Iterate through each list of ranked documents
    for docs in results:
        for rank, doc in enumerate(docs):
            doc_str = dumps(doc)
            if doc_str not in fused_scores:
                fused_scores[doc_str] = 0
            # The core of RRF: documents ranked higher (lower rank value) get a larger score
            fused_scores[doc_str] += 1 / (rank + k)

    # Sort documents by their new fused scores in descending order
    reranked_results = [
        (loads(doc), score)
        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)
    ]
    return reranked_results

"""The above function will re-rank the documents after they are fetched through similarity search, but we haven’t initialized it yet so let’s do that now."""

# Use a slightly different prompt for RAG-Fusion
template = """You are a helpful assistant that generates multiple search queries based on a single input query. \n
Generate multiple search queries related to: {question} \n
Output (4 queries):"""
prompt_rag_fusion = ChatPromptTemplate.from_template(template)

generate_queries = (
    prompt_rag_fusion
    | llm
    | StrOutputParser()
    | (lambda x: x.split("\n"))
)

# Build the new retrieval chain with RRF
retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion
docs = retrieval_chain_rag_fusion.invoke({"question": question})

print(f"Total re-ranked documents retrieved: {len(docs)}")

"""The final chain remains the same, but now it receives a more intelligently ranked context. RAG-Fusion is a powerful, low-effort way to increase the quality of your retrieval.

<a id='part2-3'></a>
## Decomposition

Some questions are too complex to be answered in a single step. For example, **“What are the main components of an LLM-powered agent, and how do they interact?”** This is really two questions in one.

![Answer Recursively (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*oYttQUN_G0J_TZtigWjsGQ.png)

The Decomposition technique uses an LLM to break down a complex query into a set of simpler, self-contained sub-questions. We can then answer each one and synthesize a final answer.

We’ll start with a prompt designed for this purpose.
"""

# Decomposition prompt
template = """You are a helpful assistant that generates multiple sub-questions related to an input question. \n
The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \n
Generate multiple search queries related to: {question} \n
Output (3 queries):"""
prompt_decomposition = ChatPromptTemplate.from_template(template)

llm = ChatGroq(
    model="deepseek-r1-distill-llama-70b",
    temperature=0,
    max_tokens=None,
    reasoning_format="parsed",
    timeout=None,
    max_retries=2
)

# Chain to generate sub-questions
generate_queries_decomposition = (
    prompt_decomposition
    | llm
    | StrOutputParser()
    | (lambda x: x.split("\n"))
)

# Generate and print the sub-questions
question = "What are leave and performance review policies at uptiq?"
sub_questions = generate_queries_decomposition.invoke({"question": question})
print(sub_questions)

"""The LLM successfully decomposed our complex question. Now, we can answer each of these individually and combine the results. One effective method is to answer each sub-question and use the resulting Q&A pairs as context to synthesize a final, comprehensive answer."""

# RAG prompt
prompt_rag = hub.pull("rlm/rag-prompt")

# A list to hold the answers to our sub-questions
rag_results = []
for sub_question in sub_questions:
    # Retrieve documents for each sub-question
    retrieved_docs = retriever.get_relevant_documents(sub_question)

    # Use our standard RAG chain to answer the sub-question
    answer = (prompt_rag | llm | StrOutputParser()).invoke({"context": retrieved_docs, "question": sub_question})
    rag_results.append(answer)

def format_qa_pairs(questions, answers):
    """Format Q and A pairs"""
    formatted_string = ""
    for i, (question, answer) in enumerate(zip(questions, answers), start=1):
        formatted_string += f"Question {i}: {question}\nAnswer {i}: {answer}\n\n"
    return formatted_string.strip()

# Format the Q&A pairs into a single context string
context = format_qa_pairs(sub_questions, rag_results)

# Final synthesis prompt
template = """Here is a set of Q+A pairs:

{context}

Use these to synthesize an answer to the original question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

final_rag_chain = (
    prompt
    | llm
    | StrOutputParser()
)

print(final_rag_chain.invoke({"context": context, "question": question}))

"""By breaking the problem down, we constructed a much more detailed and structured answer than we would have otherwise.

<a id='part2-4'></a>
## Step-Back Prompting

Sometimes, a user’s query is too specific, while our documents contain the more general, underlying information needed to answer it.

![Step Back Prompting (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:875/1*6lrhGv1fdcmLKMVu5tU3uQ.png)

> For example, a user might ask, “Could the members of The Police perform lawful arrests?”

A direct search for this might fail. The Step-Back technique uses an LLM to take a “step back” and form a more general question, like “What are the powers and duties of the band The Police?” We then retrieve context for *both* the specific and general questions, providing a richer context for the final answer.

We can teach the LLM this pattern using few-shot examples.
"""

from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate

# Few-shot examples to teach the model how to generate step-back (more generic) questions
examples = [
    {
        "input": "Can I carry forward 8 unused annual leave days into the next year?",
        "output": "What is Uptiq’s policy on carrying forward unused annual leave?",
    },
    {
        "input": "Do I get reimbursed if I buy my own Wi-Fi router while working from home?",
        "output": "What expenses are reimbursed under Uptiq’s Work From Home policy?",
    },
]

# Define how each example is formatted in the prompt
example_prompt = ChatPromptTemplate.from_messages([
    ("human", "{input}"),  # User input
    ("ai", "{output}")     # Model's response
])

# Wrap the few-shot examples into a reusable prompt template
few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples,
)

# Full prompt includes system instruction, few-shot examples, and the user question
prompt = ChatPromptTemplate.from_messages([
    ("system",
     "You are an AI assistant trained on HR policies of Uptiq. Your task is to step back and paraphrase a question "
     "to a more generic step-back question, which is easier to answer. Here are a few examples:"),
    few_shot_prompt,
    ("user", "{question}"),
])

"""Now, we can simply define the chain for step back approach, so let’s do that."""

# Define a chain to generate step-back questions using the prompt and an OpenAI model
generate_queries_step_back = prompt | llm | StrOutputParser()

# Run the chain on a specific question
question = "What are leave and performance review policies at uptiq?"
step_back_question = generate_queries_step_back.invoke({"question": question})

# Output the original and generated step-back question
print(f"Original Question: {question}")
print(f"Step-Back Question: {step_back_question}")

"""This is an important step-back question. It broadens the scope to general software engineering, which will likely pull in foundational documents that can then be combined with the specific context about LLM agents. Now we can build a chain that uses both."""

from langchain_core.runnables import RunnableLambda

# Prompt for the final response
response_prompt_template = """You are an AI assistant trained on HR policies of Uptiq. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.

# Normal Context
{normal_context}

# Step-Back Context
{step_back_context}

# Original Question: {question}
# Answer:"""
response_prompt = ChatPromptTemplate.from_template(response_prompt_template)

# The full chain
chain = (
    {
        # Retrieve context using the normal question
        "normal_context": RunnableLambda(lambda x: x["question"]) | retriever,
        # Retrieve context using the step-back question
        "step_back_context": generate_queries_step_back | retriever,
        # Pass on the original question
        "question": lambda x: x["question"],
    }
    | response_prompt
    | llm
    | StrOutputParser()
)

response = chain.invoke({"question": question})

"""This is the output we get, when we run this step back prompt chain with our query."""

print(response)

"""<a id='part2-5'></a>
## HyDE

This final technique is one of the most clever. The core problem of retrieval is that a user’s query might use different words than the document (the “vocabulary mismatch” problem).

![HyDE (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*YQVJMOpDBU6l54atHoFpJg.png)

**HyDE (Hypothetical Document Embeddings)** proposes a radical solution: First, have an LLM generate a *hypothetical* answer to the question. This fake document, while not factually correct, will be semantically rich and use the kind of language we expect to find in a real answer.

We then embed this hypothetical document and use its embedding to perform the retrieval. The result is that we find real documents that are semantically very similar to an ideal answer.

Let’s start by creating a prompt to generate this hypothetical document.
"""

question = "Do I get reimbursed for electricity bills if I work from home?"

# HyDE prompt
template = """Please write a scientific paper passage to answer the question
Question: {question}
Passage:"""
prompt_hyde = ChatPromptTemplate.from_template(template)

# Chain to generate the hypothetical document
generate_docs_for_retrieval = (
    prompt_hyde
    | llm
    | StrOutputParser()
)

# Generate and print the hypothetical document
hypothetical_document = generate_docs_for_retrieval.invoke({"question": question})
print(hypothetical_document)

"""This passage is a perfect, textbook-style answer. Now, we use its embedding to find real documents."""

# Retrieve documents using the HyDE approach
retrieval_chain = generate_docs_for_retrieval | retriever
retrieved_docs = retrieval_chain.invoke({"question": question})

# Use our standard RAG chain to generate the final answer from the retrieved context
response = final_rag_chain.invoke({"context": retrieved_docs, "question": question})
print(response)

"""By using a hypothetical document as a **lure**, HyDE helped us zero in on the most relevant chunks in our knowledge base, demonstrating another powerful tool in our RAG toolkit.

<a id='part3'></a>
# Routing & Query Construction

Our RAG system is getting smarter, but in a real-world scenario, knowledge isn’t stored in a single, uniform library.

> We often have multiple data sources: documentation for different programming languages, internal wikis, public websites, or databases with structured metadata.

![Routing and Query Transformation (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*cost0_AWB8NKp0WxZlH7fA.png)

Sending every query to every source is wildly inefficient and can lead to noisy, irrelevant results.

This is where our RAG system needs to evolve from a simple librarian into an **intelligent switchboard operator**. It needs the ability to first *analyze* an incoming query and then *route* it to the correct destination or *construct* a more precise, structured query for retrieval. This section dives into the techniques that make this possible.

<a id='part3-1'></a>
## Logical Routing

Routing is a classification problem. Given a user’s question, we need to classify it into one of several predefined categories. While traditional ML models can do this, we can leverage the powerful reasoning engine we already have: the LLM itself.

![Logical Routing (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:875/1*PK9xKW0o-72xmmLaAAozeA.png)

By providing the LLM with a clear schema (a set of possible categories), we can ask it to make the classification decision for us.

We’ll start by defining the “contract” for our LLM’s output using a Pydantic model. This schema explicitly tells the LLM the possible destinations for a query.
"""

# from typing import Literal
# from langchain_core.pydantic_v1 import BaseModel, Field

# # Define the data model for our router's output
# class RouteQuery(BaseModel):
#     """A data model to route a user query to the most relevant datasource."""

#     # The 'datasource' field must be one of the three specified literal strings.
#     # This enforces a strict set of choices for the LLM.
#     datasource: Literal["python_docs", "js_docs", "golang_docs"] = Field(
#         ...,  # The '...' indicates that this field is required.
#         description="Given a user question, choose which datasource would be most relevant for answering their question.",
#     )

from typing import Literal
from langchain_core.pydantic_v1 import BaseModel, Field

# Define the data model for routing based on HR policy files
class RouteQuery(BaseModel):
    """A data model to route a user query to the most relevant HR policy file."""

    # The 'file_name' field must be one of the HR policy files.
    file_name: Literal[
        "employee_code_of_conduct.txt",
        "leave_policy.txt",
        "work_from_home_policy.txt",
        "payroll_and_compensation_policy.txt",
        "performance_review_policy.txt",
        "it_and_security_policy.txt"
    ] = Field(
        ...,
        description=(
            "Given a user question, choose which HR policy file "
            "would be most relevant for answering their question."
        ),
    )

"""With our schema defined, we can now build the router chain. We’ll use a prompt to give the LLM its instructions and then use the `.with_structured_output()` method to ensure its response perfectly matches our `RouteQuery` model."""

# Initialize our LLM
# llm = ChatOpenAI(model="gpt-3.5-turbo-0125", temperature=0)

# Create a new LLM instance that is "structured" to output our Pydantic model
structured_llm = llm.with_structured_output(RouteQuery)

# The system prompt provides the core instruction for the LLM's task.
# system = """You are an expert at routing a user question to the appropriate data source.

# Based on the programming language the question is referring to, route it to the relevant data source."""

system = """You are an expert at routing a user question to the appropriate data source.
Given a user question, choose which HR policy file would be most relevant for answering their question."""

# The full prompt template combines the system message and the user's question.
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "{question}"),
    ]
)

# Define the complete router chain
router = prompt | structured_llm

"""Now, let’s test our router. We’ll pass it a question that is clearly about Python and inspect the output."""

# Example user queries
questions = [
    "Can I carry forward my annual leave?",                         # leave_policy.txt
    "What should I do if my company laptop gets stolen?",            # it_and_security_policy.txt
    "Am I allowed to wear jeans for a client meeting?",              # employee_code_of_conduct.txt
    "How often does Uptiq conduct performance reviews?"              # performance_review_policy.txt
]

# Invoke the router for each query and print results
for q in questions:
    result = router.invoke({"question": q})
    print(f"User Question: {q}")
    print(f"Routed to File: {result.file_name}\n")

"""The output is an instance of our `RouteQuery` model, and the LLM has correctly identified `python_docs` as the appropriate datasource. This structured output is now something we can reliably use in our code to implement branching logic."""

# from langchain_core.runnables import RunnableLambda

# def choose_route(result):
#     """A function to determine the downstream logic based on the router's output."""
#     if "python_docs" in result.datasource.lower():
#         # In a real app, this would be a complete RAG chain for Python docs
#         return "chain for python_docs"
#     elif "js_docs" in result.datasource.lower():
#         # This would be the chain for JavaScript docs
#         return "chain for js_docs"
#     else:
#         # And this for Go docs
#         return "chain for golang_docs"

# # The full chain now includes the routing and branching logic
# full_chain = router | RunnableLambda(choose_route)

# # Let's run the full chain
# final_destination = full_chain.invoke({"question": question})

# print(final_destination)

"""Our switchboard correctly routed the Python-related query. This approach is incredibly powerful for building multi-source RAG systems.

<a id='part3-2'></a>
## Semantic Routing

Logical routing works perfectly when you have clearly defined categories. But what if you want to route based on the *style* or *domain* of a question? For example, you might want to answer physics questions with a serious, academic tone and math questions with a step-by-step, pedagogical approach. This is where **Semantic Routing** comes in.

![Semantic Routing (Created by Fareed Khan)](https://miro.medium.com/v2/resize:fit:1250/1*mzz-ncmrzdwQU37GFgPeTw.png)

> Instead of classifying the query, we define multiple expert prompts.

We then embed the user’s query and each of our prompt templates, and use cosine similarity to find the prompt that is most semantically aligned with the query.

First, let’s define our two expert personas.
"""

from langchain_core.prompts import PromptTemplate

# A prompt for a hr expert
hr_template = """You are an HR policies assistant for the company Uptiq.
    You answer questions about leave policies, payroll, employee benefits, and workplace compliance.
    Always explain clearly and reference HR rules.

    Here is a question:
{query}"""

# A prompt for a it expert
it_template = """You are an IT helpdesk expert.
    You answer questions related to technical troubleshooting, software, hardware,
    network issues, and security guidelines in a simple and actionable way.

    Here is a question:
{query}"""


# A prompt for a law expert
law_template = """You are a legal advisor.
    You explain laws, regulations, workplace compliance, contracts, and employee rights
    in a clear and simple manner.
    Always include a disclaimer that this is not professional legal advice.

    Here is a question:
{query}"""

"""Now, we’ll create the routing function that performs the embedding and similarity comparison."""

from langchain.utils.math import cosine_similarity
from langchain_huggingface import HuggingFaceEmbeddings

# Initialize the embedding model
embeddings = HuggingFaceEmbeddings(
        model_name="all-MiniLM-L6-v2"  # Fast and efficient model
        # model_name="all-mpnet-base-v2"  # Better quality but slower
    )


# Store our templates and their embeddings for comparison
prompt_templates = [hr_template, it_template, law_template]
prompt_embeddings = embeddings.embed_documents(prompt_templates)

def prompt_router(input):
    """A function to route the input query to the most similar prompt template."""
    # 1. Embed the incoming user query
    query_embedding = embeddings.embed_query(input["query"])

    # 2. Compute the cosine similarity between the query and all prompt templates
    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]

    # 3. Find the index of the most similar prompt
    most_similar_index = similarity.argmax()

    template_names = ["hr_template", "law_template", "it_template"]
    choosen_name = template_names[most_similar_index]

    # 4. Select the most similar prompt template
    chosen_prompt = prompt_templates[most_similar_index]

    print(f"DEBUG: Routed to {choosen_name} template.")

    # 5. Return the chosen prompt object
    return PromptTemplate.from_template(chosen_prompt)

"""With the routing logic in place, we can build the full chain that dynamically selects the right expert for the job."""

# The final chain that combines the router with the LLM
chain = (
    {"query": RunnablePassthrough()}
    | RunnableLambda(prompt_router)  # Dynamically select the prompt
    | llm
    | StrOutputParser()
)
# pip freeze > requirements-dev.txt
# Ask a physics question
print(chain.invoke("What should I do if my company laptop gets stolen?"))

"""Perfect. The router correctly identified the question as physics-related and used the physics professor prompt, resulting in a concise and accurate answer. This technique is excellent for creating specialized agents that adapt their persona to the user’s needs."""

# pip freeze > requirements-dev.txt

# pip install pip-chill

# !pip-chill > requirements.txt
